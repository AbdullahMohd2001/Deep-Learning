{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSTrUzIc2rpjwMxGIWwHZ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahMohd2001/Deep-Learning/blob/main/DL_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 1**\n",
        "\n",
        "The reason that we have used SIGMOID(IN_VALUE) instead of PREDICT(WEIGHTS, INSTANCE) is the fact that it lies within the code used. The reasons can be listed below:\n",
        "1. Output Calculation: the perceptron uses in the training process is to compute the result of applying sigmoid function to the dot product that the program has in it. the output which we get throught this is a continuous value which ranges between 0 and 1 due to the sigmoid function.\n",
        "2. Error Calculation: It is based on the continuous output which is obtained from the given function. this is used to represent the differrence between the predicted output and the actual label of the instance.\n",
        "3. Predict Function: this function is primarily used for making predictions after the model has been trained where it applies a threshold to the output to obtain a binary prediction.\n",
        "\n",
        "So, the SIGMOID(IN_VALUE) is used to compute the output during the training process which allows for a smoother optimization process by having continuous values."
      ],
      "metadata": {
        "id": "T0CiduJ5sfwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 2**"
      ],
      "metadata": {
        "id": "FfSXKJaJspv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A88Zj-QZcrEg",
        "outputId": "0fb9feca-68cf-472f-e052-80b9c57a4f8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-19 04:58:16--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat.1’\n",
            "\n",
            "test.dat.1          100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-19 04:58:16 (88.7 MB/s) - ‘test.dat.1’ saved [2844/2844]\n",
            "\n",
            "--2024-02-19 04:58:16--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat.1’\n",
            "\n",
            "train.dat.1         100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-19 04:58:16 (136 MB/s) - ‘train.dat.1’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "c50e6df7-5cb5-463a-9750-253342d2df94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "e95e8946-9d2b-4e99-ffa7-ee67d562c2b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "def read_data(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        data = np.loadtxt(f)\n",
        "    return data\n",
        "\n",
        "def train_perceptron(instances, learning_rate, epochs):\n",
        "    # Create a Perceptron instance\n",
        "    perceptron = Perceptron(eta0=learning_rate)\n",
        "\n",
        "    # Train the perceptron on the given instances\n",
        "    perceptron.fit(instances)\n",
        "\n",
        "    # Return the weights of the perceptron\n",
        "    return perceptron.coef_\n",
        "\n",
        "def get_accuracy(weights, instances):\n",
        "    # Calculate the number of correct predictions\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Iterate over the instances\n",
        "    for instance in instances:\n",
        "        # Calculate the predicted label\n",
        "        predicted_label = int(np.dot(weights, instance[:-1]) >= 0)\n",
        "\n",
        "        # Check if the predicted label matches the true label\n",
        "        if predicted_label == instance[-1]:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate and return the accuracy\n",
        "    return correct_predictions / len(instances)"
      ],
      "metadata": {
        "id": "6RUuIUI5d3rw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "CUSBQLjkcGgd",
        "outputId": "3bae651d-f876-477b-a5ce-ff8ebaed9cff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-25335c47083f>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpercent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_percent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Calculate the number of instances to use based on tr_percent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mnum_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpercent\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0minstances_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstances_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_instances\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Take the subset of instances for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "\n",
        "# Define tr_percent, num_epochs, and lr\n",
        "tr_percent = [5, 10, 25, 50, 75, 100]\n",
        "num_epochs = [5, 10, 20, 50, 100]\n",
        "lr = [0.005, 0.01, 0.05]\n",
        "\n",
        "# Initialize variables to store best hyperparameters and accuracy\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "\n",
        "# Iterate over all combinations of hyperparameters\n",
        "for percent, epochs, learning_rate in itertools.product(tr_percent, num_epochs, lr):\n",
        "    # Calculate the number of instances to use based on tr_percent\n",
        "    num_instances = len(instances_tr) * percent // 100\n",
        "    instances_subset = instances_tr[:num_instances]  # Take the subset of instances for training\n",
        "\n",
        "    # Train the perceptron\n",
        "    weights = train_perceptron(instances_subset, learning_rate, epochs)\n",
        "\n",
        "    # Calculate accuracy on the test dataset\n",
        "    accuracy = get_accuracy(weights, instances_te)\n",
        "\n",
        "    # Check if current accuracy is better than the best accuracy found so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_hyperparameters = {'tr_percent': percent, 'num_epochs': epochs, 'lr': learning_rate}\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_hyperparameters)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTION 3**\n",
        "\n",
        "3A. The relationship between the size of the training dataset (tr_percent), the number of epochs (num_epochs), the learning rate (lr), and the resulting accuracy on the test dataset must first be examined in order to interpret the results of training the perceptron with various combinations of hyperparameters.\n",
        "This suggests that adopting a smaller subset of the training data might, in certain circumstances, result in improved generalization and avoid overfitting to the training set.\n",
        "We may make a heatmap to show this connection, with the color intensity representing the accuracy on the test dataset, the y-axis representing the learning rate, and the x-axis representing the number of epochs. We can see how the ideal hyperparameters vary with the quantity of the data by charting this heatmap for various values of tr_percent.\n",
        "\n",
        "\n",
        "*3B.* Despite utilizing additional training data, there are a few possible explanations for the second run's lower accuracy than the first:\n",
        "\n",
        "**Overfitting:** The training dataset size (tr_percent) may have increased from 100 to 200, which might have made the model more prone to overfitting.\n",
        "\n",
        "**Learning Rate:** Choosing the appropriate learning rate (lr) has a big influence on how training goes. The learning rate was lowered to 0.005 in the second run from 0.05.\n",
        "\n",
        "**Number of Epochs:** Despite the fact that there were exactly 20 epochs in each run, the effect of this parameter can still differ based on a number of other variables, including dataset size and learning rate.\n",
        "Variability of the Dataset: The representativeness and variability of the training dataset must be taken into account.\n",
        "\n",
        "\n",
        "*3C.*In order to get an accuracy level greater than 80.0 on the test dataset, we may investigate other hyperparameters and perhaps modify the current ones. The following are some tactics to think about:\n",
        "\n",
        "**Model Intricacy:**\n",
        "We may experiment with more intricate model architectures, including multi-layer perceptrons or deep learning models, which are neural networks with several layers.\n",
        "\n",
        "**Engineering Features:**\n",
        "To extract more enlightening characteristics from the data, we can investigate feature engineering strategies.\n",
        "\n",
        "**Regularization:**\n",
        "By using regularization strategies like dropout, early halting, or L1 or L2 regularization, overfitting may be avoided and generalization performance can be enhanced.\n",
        "\n",
        "**Ensemble Learning:**\n",
        "You may use ensemble learning techniques like stacking, boosting, and bagging to combine several basic models and get predictions that are more accurate.\n",
        "\n",
        "\n",
        "*3D.* The model's performance can be enhanced by training for additional epochs, although doing so isn't always worthwhile, particularly if other hyperparameters aren't changed appropriately. The following are some things to think about:\n",
        "\n",
        "**Reduction in Returns**\n",
        "There may be a decreasing return on performance gain when the number of epochs is increased. More training epochs may initially help the model as it gains the ability to recognize increasingly intricate patterns in the data.\n",
        "\n",
        "**Calculation Cost:**\n",
        "More time and processing power are needed to train for more epochs. It might not be worthwhile to train for longer epochs if the marginal performance gain is negligible in comparison to the computing expense.\n",
        "\n",
        "**Terminating Early:**\n",
        "Determining the ideal number of epochs and preventing overfitting can be accomplished by utilizing early halting approaches.\n",
        "\n",
        "**Hyperparameter Interaction:**\n",
        "The learning rate and regularization intensity are two hyperparameters that interact with the number of epochs. To maintain optimal performance, modifying additional hyperparameters could be necessary in addition to changing the number of epochs.\n"
      ],
      "metadata": {
        "id": "XiEJV4jwsvFq"
      }
    }
  ]
}